{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create env\n",
    "# python3 -m venv .venv\n",
    "# source .venv/bin/activate\n",
    "# pip install torch transformers pandas datasets \"transformers[toruch]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khaled/Documents/StoryTeller/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/khaled/Documents/StoryTeller/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer , AutoModelForCausalLM\n",
    "model_name = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Once upon a time, we find that there is no place to stand, in a place of beauty, with this kind of beauty, that exists.\"\\n\\n\"How does the world compare with this?\\n\"Let me tell you,\" she exclaimed. \"It is not.\"\\n\"A better word, dear, we can all laugh, you know, and laugh with dignity, in our day, but in the present, we are living in a way that we have not yet, of necessity.\"<|endoftext|>']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Once upon a time\"\n",
    "\n",
    "inputs = tokenizer(prompt , return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(inputs.input_ids,max_new_tokens=100,do_sample=True,top_k=50,top_p=0.95)\n",
    "\n",
    "outputs_string = tokenizer.batch_decode(outputs)\n",
    "\n",
    "outputs_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 2119719\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 21990\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "short_stories_dataset = load_dataset(\"roneneldan/TinyStories\")\n",
    "short_stories_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\\n\\nLily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\\n\\nTogether, they shared the needle and sewed the button on Lily\\'s shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_stories_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 800\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a smaller data\n",
    "\n",
    "samll_story_dataset =  load_dataset(\"roneneldan/TinyStories\",split=\"train[:1000]\")\n",
    "samll_story_dataset = samll_story_dataset.train_test_split(train_size=0.8)\n",
    "\n",
    "samll_story_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[199,\n",
       " 132,\n",
       " 215,\n",
       " 134,\n",
       " 330,\n",
       " 164,\n",
       " 232,\n",
       " 103,\n",
       " 325,\n",
       " 89,\n",
       " 166,\n",
       " 119,\n",
       " 198,\n",
       " 128,\n",
       " 137,\n",
       " 141,\n",
       " 179,\n",
       " 119,\n",
       " 140,\n",
       " 108,\n",
       " 137,\n",
       " 160,\n",
       " 163,\n",
       " 190,\n",
       " 166,\n",
       " 149,\n",
       " 145,\n",
       " 196,\n",
       " 109,\n",
       " 342,\n",
       " 146,\n",
       " 141,\n",
       " 196,\n",
       " 179,\n",
       " 142,\n",
       " 140,\n",
       " 335,\n",
       " 178,\n",
       " 116,\n",
       " 129,\n",
       " 165,\n",
       " 135,\n",
       " 131,\n",
       " 138,\n",
       " 138,\n",
       " 158,\n",
       " 142,\n",
       " 125,\n",
       " 144,\n",
       " 134,\n",
       " 265,\n",
       " 215,\n",
       " 159,\n",
       " 200,\n",
       " 246,\n",
       " 184,\n",
       " 141,\n",
       " 203,\n",
       " 214,\n",
       " 86,\n",
       " 462,\n",
       " 435,\n",
       " 119,\n",
       " 192,\n",
       " 441,\n",
       " 164,\n",
       " 293,\n",
       " 374,\n",
       " 94,\n",
       " 183,\n",
       " 317,\n",
       " 129,\n",
       " 151,\n",
       " 271,\n",
       " 127,\n",
       " 270,\n",
       " 158,\n",
       " 94,\n",
       " 299,\n",
       " 381,\n",
       " 244,\n",
       " 140,\n",
       " 130,\n",
       " 142,\n",
       " 114,\n",
       " 127,\n",
       " 332,\n",
       " 176,\n",
       " 164,\n",
       " 262,\n",
       " 150,\n",
       " 150,\n",
       " 162,\n",
       " 138,\n",
       " 139,\n",
       " 138,\n",
       " 157,\n",
       " 66,\n",
       " 140,\n",
       " 252,\n",
       " 318,\n",
       " 309,\n",
       " 215,\n",
       " 157,\n",
       " 161,\n",
       " 187,\n",
       " 161,\n",
       " 319,\n",
       " 163,\n",
       " 341,\n",
       " 159,\n",
       " 116,\n",
       " 109,\n",
       " 150,\n",
       " 102,\n",
       " 116,\n",
       " 190,\n",
       " 144,\n",
       " 111,\n",
       " 149,\n",
       " 125,\n",
       " 137,\n",
       " 231,\n",
       " 110,\n",
       " 141,\n",
       " 171,\n",
       " 107,\n",
       " 82,\n",
       " 210,\n",
       " 158,\n",
       " 320,\n",
       " 136,\n",
       " 161,\n",
       " 91,\n",
       " 106,\n",
       " 168,\n",
       " 131,\n",
       " 116,\n",
       " 267,\n",
       " 193,\n",
       " 119,\n",
       " 112,\n",
       " 183,\n",
       " 198,\n",
       " 170,\n",
       " 323,\n",
       " 136,\n",
       " 153,\n",
       " 143,\n",
       " 130,\n",
       " 149,\n",
       " 133,\n",
       " 164,\n",
       " 133,\n",
       " 147,\n",
       " 189,\n",
       " 183,\n",
       " 176,\n",
       " 112,\n",
       " 188,\n",
       " 181,\n",
       " 106,\n",
       " 165,\n",
       " 84,\n",
       " 305,\n",
       " 129,\n",
       " 123,\n",
       " 90,\n",
       " 147,\n",
       " 137,\n",
       " 318,\n",
       " 150,\n",
       " 240,\n",
       " 107,\n",
       " 162,\n",
       " 328,\n",
       " 149,\n",
       " 202,\n",
       " 286,\n",
       " 137,\n",
       " 132,\n",
       " 173,\n",
       " 110,\n",
       " 198,\n",
       " 166,\n",
       " 128,\n",
       " 106,\n",
       " 155,\n",
       " 72,\n",
       " 583,\n",
       " 116,\n",
       " 171,\n",
       " 112,\n",
       " 241,\n",
       " 133,\n",
       " 211,\n",
       " 165,\n",
       " 174,\n",
       " 159,\n",
       " 143,\n",
       " 151,\n",
       " 177,\n",
       " 151,\n",
       " 365,\n",
       " 132,\n",
       " 128,\n",
       " 176,\n",
       " 134,\n",
       " 145,\n",
       " 137,\n",
       " 188,\n",
       " 177,\n",
       " 132,\n",
       " 354,\n",
       " 280,\n",
       " 118,\n",
       " 162,\n",
       " 218,\n",
       " 94,\n",
       " 135,\n",
       " 137,\n",
       " 159,\n",
       " 138,\n",
       " 407,\n",
       " 226,\n",
       " 138,\n",
       " 285,\n",
       " 151,\n",
       " 194,\n",
       " 154,\n",
       " 112,\n",
       " 194,\n",
       " 125,\n",
       " 151,\n",
       " 120,\n",
       " 169,\n",
       " 120,\n",
       " 125,\n",
       " 115,\n",
       " 272,\n",
       " 144,\n",
       " 176,\n",
       " 145,\n",
       " 111,\n",
       " 172,\n",
       " 167,\n",
       " 203,\n",
       " 120,\n",
       " 105,\n",
       " 232,\n",
       " 121,\n",
       " 156,\n",
       " 141,\n",
       " 153,\n",
       " 205,\n",
       " 115,\n",
       " 162,\n",
       " 176,\n",
       " 186,\n",
       " 106,\n",
       " 149,\n",
       " 120,\n",
       " 150,\n",
       " 107,\n",
       " 236,\n",
       " 118,\n",
       " 184,\n",
       " 125,\n",
       " 272,\n",
       " 136,\n",
       " 188,\n",
       " 141,\n",
       " 132,\n",
       " 119,\n",
       " 227,\n",
       " 177,\n",
       " 167,\n",
       " 141,\n",
       " 157,\n",
       " 120,\n",
       " 162,\n",
       " 276,\n",
       " 155,\n",
       " 132,\n",
       " 206,\n",
       " 285,\n",
       " 165,\n",
       " 155,\n",
       " 157,\n",
       " 101,\n",
       " 129,\n",
       " 140,\n",
       " 90,\n",
       " 150,\n",
       " 305,\n",
       " 119,\n",
       " 144,\n",
       " 149,\n",
       " 103,\n",
       " 166,\n",
       " 197,\n",
       " 364,\n",
       " 128,\n",
       " 135,\n",
       " 168,\n",
       " 150,\n",
       " 151,\n",
       " 130,\n",
       " 255,\n",
       " 378,\n",
       " 164,\n",
       " 204,\n",
       " 140,\n",
       " 123,\n",
       " 99,\n",
       " 114,\n",
       " 154,\n",
       " 329,\n",
       " 116,\n",
       " 276,\n",
       " 189,\n",
       " 129,\n",
       " 182,\n",
       " 119,\n",
       " 182,\n",
       " 94,\n",
       " 152,\n",
       " 151,\n",
       " 194,\n",
       " 126,\n",
       " 220,\n",
       " 133,\n",
       " 215,\n",
       " 143,\n",
       " 141,\n",
       " 220,\n",
       " 146,\n",
       " 151,\n",
       " 111,\n",
       " 193,\n",
       " 121,\n",
       " 124,\n",
       " 263,\n",
       " 144,\n",
       " 229,\n",
       " 147,\n",
       " 107,\n",
       " 171,\n",
       " 99,\n",
       " 140,\n",
       " 185,\n",
       " 152,\n",
       " 95,\n",
       " 144,\n",
       " 277,\n",
       " 141,\n",
       " 130,\n",
       " 139,\n",
       " 144,\n",
       " 112,\n",
       " 137,\n",
       " 108,\n",
       " 343,\n",
       " 142,\n",
       " 155,\n",
       " 318,\n",
       " 194,\n",
       " 196,\n",
       " 178,\n",
       " 124,\n",
       " 366,\n",
       " 173,\n",
       " 300,\n",
       " 154,\n",
       " 315,\n",
       " 181,\n",
       " 323,\n",
       " 141,\n",
       " 414,\n",
       " 165,\n",
       " 207,\n",
       " 132,\n",
       " 175,\n",
       " 137,\n",
       " 164,\n",
       " 147,\n",
       " 418,\n",
       " 113,\n",
       " 97,\n",
       " 136,\n",
       " 191,\n",
       " 109,\n",
       " 161,\n",
       " 158,\n",
       " 133,\n",
       " 182,\n",
       " 141,\n",
       " 117,\n",
       " 135,\n",
       " 276,\n",
       " 113,\n",
       " 177,\n",
       " 243,\n",
       " 116,\n",
       " 114,\n",
       " 158,\n",
       " 147,\n",
       " 146,\n",
       " 120,\n",
       " 131,\n",
       " 144,\n",
       " 298,\n",
       " 129,\n",
       " 179,\n",
       " 145,\n",
       " 196,\n",
       " 181,\n",
       " 413,\n",
       " 147,\n",
       " 122,\n",
       " 247,\n",
       " 159,\n",
       " 140,\n",
       " 189,\n",
       " 194,\n",
       " 201,\n",
       " 159,\n",
       " 161,\n",
       " 158,\n",
       " 401,\n",
       " 304,\n",
       " 141,\n",
       " 140,\n",
       " 301,\n",
       " 135,\n",
       " 142,\n",
       " 108,\n",
       " 216,\n",
       " 104,\n",
       " 188,\n",
       " 493,\n",
       " 155,\n",
       " 160,\n",
       " 164,\n",
       " 130,\n",
       " 204,\n",
       " 123,\n",
       " 139,\n",
       " 105,\n",
       " 197,\n",
       " 148,\n",
       " 155,\n",
       " 105,\n",
       " 367,\n",
       " 110,\n",
       " 141,\n",
       " 233,\n",
       " 144,\n",
       " 181,\n",
       " 212,\n",
       " 260,\n",
       " 154,\n",
       " 95,\n",
       " 296,\n",
       " 212,\n",
       " 135,\n",
       " 146,\n",
       " 177,\n",
       " 112,\n",
       " 147,\n",
       " 140,\n",
       " 184,\n",
       " 124,\n",
       " 132,\n",
       " 118,\n",
       " 139,\n",
       " 61,\n",
       " 195,\n",
       " 156,\n",
       " 172,\n",
       " 164,\n",
       " 86,\n",
       " 272,\n",
       " 170,\n",
       " 188,\n",
       " 196,\n",
       " 171,\n",
       " 179,\n",
       " 253,\n",
       " 104,\n",
       " 162,\n",
       " 122,\n",
       " 138,\n",
       " 141,\n",
       " 447,\n",
       " 298,\n",
       " 462,\n",
       " 137,\n",
       " 189,\n",
       " 151,\n",
       " 164,\n",
       " 150,\n",
       " 161,\n",
       " 123,\n",
       " 596,\n",
       " 126,\n",
       " 167,\n",
       " 150,\n",
       " 109,\n",
       " 187,\n",
       " 150,\n",
       " 126,\n",
       " 192,\n",
       " 136,\n",
       " 134,\n",
       " 174,\n",
       " 296,\n",
       " 147,\n",
       " 167,\n",
       " 176,\n",
       " 107,\n",
       " 105,\n",
       " 136,\n",
       " 127,\n",
       " 192,\n",
       " 148,\n",
       " 170,\n",
       " 117,\n",
       " 142,\n",
       " 217,\n",
       " 134,\n",
       " 102,\n",
       " 338,\n",
       " 453,\n",
       " 129,\n",
       " 131,\n",
       " 397,\n",
       " 220,\n",
       " 111,\n",
       " 99,\n",
       " 397,\n",
       " 160,\n",
       " 338,\n",
       " 80,\n",
       " 134,\n",
       " 123,\n",
       " 152,\n",
       " 170,\n",
       " 142,\n",
       " 137,\n",
       " 172,\n",
       " 127,\n",
       " 296,\n",
       " 116,\n",
       " 202,\n",
       " 299,\n",
       " 203,\n",
       " 203,\n",
       " 294,\n",
       " 224,\n",
       " 178,\n",
       " 160,\n",
       " 107,\n",
       " 151,\n",
       " 142,\n",
       " 138,\n",
       " 175,\n",
       " 147,\n",
       " 140,\n",
       " 171,\n",
       " 218,\n",
       " 155,\n",
       " 159,\n",
       " 177,\n",
       " 155,\n",
       " 160,\n",
       " 202,\n",
       " 116,\n",
       " 160,\n",
       " 396,\n",
       " 156,\n",
       " 365,\n",
       " 136,\n",
       " 104,\n",
       " 161,\n",
       " 110,\n",
       " 156,\n",
       " 209,\n",
       " 122,\n",
       " 240,\n",
       " 215,\n",
       " 146,\n",
       " 160,\n",
       " 162,\n",
       " 130,\n",
       " 164,\n",
       " 145,\n",
       " 183,\n",
       " 191,\n",
       " 470,\n",
       " 158,\n",
       " 168,\n",
       " 143,\n",
       " 106,\n",
       " 205,\n",
       " 137,\n",
       " 138,\n",
       " 161,\n",
       " 133,\n",
       " 216,\n",
       " 112,\n",
       " 135,\n",
       " 106,\n",
       " 114,\n",
       " 208,\n",
       " 190,\n",
       " 179,\n",
       " 123,\n",
       " 125,\n",
       " 289,\n",
       " 118,\n",
       " 144,\n",
       " 219,\n",
       " 84,\n",
       " 104,\n",
       " 157,\n",
       " 268,\n",
       " 133,\n",
       " 150,\n",
       " 145,\n",
       " 139,\n",
       " 197,\n",
       " 117,\n",
       " 182,\n",
       " 148,\n",
       " 152,\n",
       " 166,\n",
       " 165,\n",
       " 115,\n",
       " 333,\n",
       " 173,\n",
       " 109,\n",
       " 142,\n",
       " 219,\n",
       " 184,\n",
       " 148,\n",
       " 92,\n",
       " 158,\n",
       " 233,\n",
       " 145,\n",
       " 142,\n",
       " 181,\n",
       " 151,\n",
       " 360,\n",
       " 156,\n",
       " 411,\n",
       " 333,\n",
       " 170,\n",
       " 129,\n",
       " 137,\n",
       " 483,\n",
       " 265,\n",
       " 322,\n",
       " 136,\n",
       " 144,\n",
       " 145,\n",
       " 150,\n",
       " 184,\n",
       " 140,\n",
       " 156,\n",
       " 146,\n",
       " 175,\n",
       " 232,\n",
       " 127,\n",
       " 156,\n",
       " 433,\n",
       " 203,\n",
       " 121,\n",
       " 137,\n",
       " 155,\n",
       " 101,\n",
       " 194,\n",
       " 150,\n",
       " 175,\n",
       " 140,\n",
       " 151,\n",
       " 115,\n",
       " 107,\n",
       " 328,\n",
       " 298,\n",
       " 154,\n",
       " 385,\n",
       " 411,\n",
       " 143,\n",
       " 262,\n",
       " 215,\n",
       " 97,\n",
       " 102,\n",
       " 78,\n",
       " 174,\n",
       " 276,\n",
       " 141,\n",
       " 226,\n",
       " 174,\n",
       " 290,\n",
       " 120,\n",
       " 136,\n",
       " 146,\n",
       " 221,\n",
       " 132,\n",
       " 134,\n",
       " 522,\n",
       " 167,\n",
       " 130,\n",
       " 115,\n",
       " 189,\n",
       " 155,\n",
       " 188,\n",
       " 157,\n",
       " 186,\n",
       " 324,\n",
       " 110,\n",
       " 395,\n",
       " 451,\n",
       " 105,\n",
       " 137,\n",
       " 221,\n",
       " 179,\n",
       " 167,\n",
       " 116,\n",
       " 155,\n",
       " 204,\n",
       " 168,\n",
       " 142,\n",
       " 522,\n",
       " 138,\n",
       " 202,\n",
       " 821,\n",
       " 110,\n",
       " 170,\n",
       " 138,\n",
       " 233,\n",
       " 183,\n",
       " 118,\n",
       " 138,\n",
       " 168,\n",
       " 163,\n",
       " 128,\n",
       " 104,\n",
       " 165,\n",
       " 149,\n",
       " 184,\n",
       " 152,\n",
       " 154,\n",
       " 158,\n",
       " 139,\n",
       " 496,\n",
       " 176,\n",
       " 107,\n",
       " 139,\n",
       " 587,\n",
       " 132,\n",
       " 125,\n",
       " 257,\n",
       " 141,\n",
       " 180,\n",
       " 134,\n",
       " 167,\n",
       " 140,\n",
       " 160,\n",
       " 164,\n",
       " 125,\n",
       " 137,\n",
       " 167,\n",
       " 206,\n",
       " 142,\n",
       " 140,\n",
       " 159,\n",
       " 151,\n",
       " 310,\n",
       " 144,\n",
       " 187,\n",
       " 123,\n",
       " 150,\n",
       " 150,\n",
       " 132,\n",
       " 202,\n",
       " 137,\n",
       " 179,\n",
       " 198,\n",
       " 151,\n",
       " 149,\n",
       " 185,\n",
       " 118,\n",
       " 109,\n",
       " 174,\n",
       " 148,\n",
       " 139,\n",
       " 144,\n",
       " 186]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(x[\"text\"].split(\" \")) for x in samll_story_dataset[\"train\"] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [00:00<00:00, 2273.43 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 1889.35 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 800\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizer dataset\n",
    "def preporocess_batch(batch):\n",
    "    all_text_items = batch[\"text\"]\n",
    "    trimed_text_items = [x[:500] for x in all_text_items]\n",
    "    return tokenizer(trimed_text_items)\n",
    "\n",
    "tokenizeed_dataset = samll_story_dataset.map(\n",
    "    preporocess_batch,\n",
    "    batched=True,\n",
    "    batch_size=10,\n",
    "    remove_columns=samll_story_dataset[\"train\"].column_names\n",
    ")\n",
    "\n",
    "tokenizeed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [7454,\n",
       "  2402,\n",
       "  257,\n",
       "  640,\n",
       "  612,\n",
       "  373,\n",
       "  257,\n",
       "  32202,\n",
       "  3706,\n",
       "  5811,\n",
       "  13,\n",
       "  679,\n",
       "  5615,\n",
       "  287,\n",
       "  257,\n",
       "  1263,\n",
       "  3881,\n",
       "  416,\n",
       "  262,\n",
       "  5417,\n",
       "  13,\n",
       "  1881,\n",
       "  1110,\n",
       "  11,\n",
       "  5811,\n",
       "  373,\n",
       "  6155,\n",
       "  1863,\n",
       "  262,\n",
       "  10481,\n",
       "  290,\n",
       "  339,\n",
       "  2497,\n",
       "  257,\n",
       "  1310,\n",
       "  2576,\n",
       "  2712,\n",
       "  13,\n",
       "  383,\n",
       "  2576,\n",
       "  3114,\n",
       "  845,\n",
       "  3772,\n",
       "  11,\n",
       "  523,\n",
       "  5811,\n",
       "  3066,\n",
       "  284,\n",
       "  12589,\n",
       "  607,\n",
       "  13,\n",
       "  198,\n",
       "  198,\n",
       "  18861,\n",
       "  531,\n",
       "  11,\n",
       "  366,\n",
       "  15496,\n",
       "  2474,\n",
       "  220,\n",
       "  198,\n",
       "  464,\n",
       "  2576,\n",
       "  13541,\n",
       "  290,\n",
       "  531,\n",
       "  11,\n",
       "  366,\n",
       "  17250,\n",
       "  612,\n",
       "  2474,\n",
       "  220,\n",
       "  198,\n",
       "  198,\n",
       "  18861,\n",
       "  1965,\n",
       "  11,\n",
       "  366,\n",
       "  2061,\n",
       "  389,\n",
       "  345,\n",
       "  1804,\n",
       "  1701,\n",
       "  220,\n",
       "  198,\n",
       "  464,\n",
       "  2576,\n",
       "  531,\n",
       "  11,\n",
       "  366,\n",
       "  40,\n",
       "  1101,\n",
       "  2045,\n",
       "  329,\n",
       "  21547,\n",
       "  12758,\n",
       "  82,\n",
       "  13,\n",
       "  1119,\n",
       "  389,\n",
       "  523,\n",
       "  2495,\n",
       "  11,\n",
       "  290,\n",
       "  6029,\n",
       "  1165,\n",
       "  2474,\n",
       "  220,\n",
       "  198,\n",
       "  198,\n",
       "  6423,\n",
       "  5811,\n",
       "  531,\n",
       "  11,\n",
       "  366,\n",
       "  40,\n",
       "  760,\n",
       "  286,\n",
       "  257,\n",
       "  1295,\n",
       "  326,\n",
       "  468,\n",
       "  6041,\n",
       "  286,\n",
       "  21547,\n",
       "  12758,\n",
       "  82,\n",
       "  11,\n",
       "  826,\n",
       "  1474,\n",
       "  616,\n",
       "  3881,\n",
       "  1363,\n",
       "  13,\n",
       "  10928,\n",
       "  345,\n",
       "  588,\n",
       "  284,\n",
       "  1282,\n",
       "  281],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizeed_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorForLanguageModeling(tokenizer=GPT2TokenizerFast(name_or_path='distilgpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "}, mlm=False, mlm_probability=0.15, pad_to_multiple_of=None, tf_experimental_compile=False, return_tensors='pt')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a data collector\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collector = DataCollatorForLanguageModeling(tokenizer=tokenizer,mlm=False)\n",
    "data_collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khaled/Documents/StoryTeller/.venv/lib/python3.12/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/khaled/Documents/StoryTeller/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:654: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "# Create out trainer \n",
    "from transformers import Trainer , TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./output\",\n",
    "    evaluation_strategy= \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=10\n",
    ")\n",
    "\n",
    "trainer = Trainer (\n",
    "    model=model,\n",
    "    train_dataset= tokenizeed_dataset[\"train\"],\n",
    "    eval_dataset= tokenizeed_dataset[\"test\"],\n",
    "    args=training_args,\n",
    "    data_collator=data_collector\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "                                                    \n",
      " 10%|â–ˆ         | 100/1000 [12:06<1:42:21,  6.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.3971805572509766, 'eval_runtime': 51.2137, 'eval_samples_per_second': 3.905, 'eval_steps_per_second': 0.488, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 20%|â–ˆâ–ˆ        | 200/1000 [24:36<1:23:56,  6.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.3008697032928467, 'eval_runtime': 49.3162, 'eval_samples_per_second': 4.055, 'eval_steps_per_second': 0.507, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 300/1000 [36:42<1:13:43,  6.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.2657206058502197, 'eval_runtime': 42.4185, 'eval_samples_per_second': 4.715, 'eval_steps_per_second': 0.589, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 400/1000 [46:51<57:57,  5.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.2425081729888916, 'eval_runtime': 40.0592, 'eval_samples_per_second': 4.993, 'eval_steps_per_second': 0.624, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 500/1000 [57:23<1:00:50,  7.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3626, 'grad_norm': 5.607207775115967, 'learning_rate': 1e-05, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 500/1000 [58:13<1:00:50,  7.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.2322754859924316, 'eval_runtime': 49.0921, 'eval_samples_per_second': 4.074, 'eval_steps_per_second': 0.509, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 600/1000 [1:09:57<42:48,  6.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.220806360244751, 'eval_runtime': 50.5336, 'eval_samples_per_second': 3.958, 'eval_steps_per_second': 0.495, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 700/1000 [1:21:41<31:45,  6.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.2151288986206055, 'eval_runtime': 53.2572, 'eval_samples_per_second': 3.755, 'eval_steps_per_second': 0.469, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 800/1000 [1:33:11<20:56,  6.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.2106552124023438, 'eval_runtime': 46.0785, 'eval_samples_per_second': 4.34, 'eval_steps_per_second': 0.543, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 900/1000 [1:44:20<10:31,  6.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.2107439041137695, 'eval_runtime': 49.0136, 'eval_samples_per_second': 4.08, 'eval_steps_per_second': 0.51, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [1:54:33<00:00,  5.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1176, 'grad_norm': 5.805267333984375, 'learning_rate': 0.0, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [1:55:22<00:00,  6.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.20931339263916, 'eval_runtime': 47.7337, 'eval_samples_per_second': 4.19, 'eval_steps_per_second': 0.524, 'epoch': 10.0}\n",
      "{'train_runtime': 6922.6626, 'train_samples_per_second': 1.156, 'train_steps_per_second': 0.144, 'train_loss': 2.2400863037109375, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=2.2400863037109375, metrics={'train_runtime': 6922.6626, 'train_samples_per_second': 1.156, 'train_steps_per_second': 0.144, 'total_flos': 286809928630272.0, 'train_loss': 2.2400863037109375, 'epoch': 10.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start train \n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model from the last checkpoints\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./output/checkpoint-500\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['happy story by Anna, Anna, Timmy, and Ben. They were having a fun day in the park. They went to the park. They saw a huge tree. The tree had a big white and purple bell.\\n\\nAnna said, \"Yup, Timmy. The bell has a name. It is Ben and Timmy. The bell is Ben and Timmy. It is Timmy. Let\\'s go to the bell.\"\\n\\nAnna and Timmy went to see the bell.']\n"
     ]
    }
   ],
   "source": [
    "prompt = \"happy story\"\n",
    "\n",
    "inputs = tokenizer(prompt , return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(inputs.input_ids,max_new_tokens=100,do_sample=True,top_k=50,top_p=0.95)\n",
    "\n",
    "outputs_string = tokenizer.batch_decode(outputs)\n",
    "\n",
    "print(outputs_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
